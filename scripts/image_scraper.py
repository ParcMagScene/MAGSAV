#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MAGSAV Image Scraper
Script de scraping d'images pour les produits MAGSAV
Recherche et t√©l√©charge les images depuis les sites des fabricants
"""

import requests
from bs4 import BeautifulSoup
import os
import sys
import json
import time
import re
import hashlib
from urllib.parse import urljoin, urlparse
from pathlib import Path
import argparse
from typing import List, Dict, Optional, Tuple

class ImageScraper:
    """Scraper d'images pour les produits MAGSAV"""
    
    def __init__(self, medias_path: str = "medias"):
        self.medias_path = Path(medias_path)
        self.photos_path = self.medias_path / "photos"
        self.scraped_path = self.medias_path / "scraped"
        
        # Cr√©er les dossiers n√©cessaires
        self.photos_path.mkdir(parents=True, exist_ok=True)
        self.scraped_path.mkdir(parents=True, exist_ok=True)
        
        # Configuration des sites de fabricants/revendeurs
        self.search_engines = {
            "google_images": "https://www.google.com/search?tbm=isch&q={query}",
            "bing_images": "https://www.bing.com/images/search?q={query}",
        }
        
        # Sites de fabricants audio/vid√©o
        self.manufacturer_sites = {
            "yamaha": {
                "base_url": "https://www.yamaha.com",
                "search_url": "https://www.yamaha.com/search?q={query}",
                "img_selectors": ["img.product-image", ".product-photo img", ".gallery img"]
            },
            "sony": {
                "base_url": "https://www.sony.com",
                "search_url": "https://www.sony.com/search?q={query}",
                "img_selectors": ["img.product-hero-image", ".product-image img", ".gallery-image"]
            },
            "panasonic": {
                "base_url": "https://www.panasonic.com",
                "search_url": "https://www.panasonic.com/search?q={query}",
                "img_selectors": ["img.product-image", ".hero-image img", ".product-gallery img"]
            },
            "bose": {
                "base_url": "https://www.bose.com",
                "search_url": "https://www.bose.com/search?q={query}",
                "img_selectors": ["img.product-image", ".hero-image", ".product-hero img"]
            }
        }
        
        # Headers pour √©viter la d√©tection de bot
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # D√©lai entre les requ√™tes (respecter les serveurs)
        self.delay_between_requests = 2
        
    def normalize_product_name(self, name: str, manufacturer: str = "") -> str:
        """Normalise le nom du produit pour la recherche"""
        # Retirer les caract√®res sp√©ciaux
        normalized = re.sub(r'[^\w\s-]', '', name)
        # Ajouter le fabricant si fourni
        if manufacturer:
            normalized = f"{manufacturer} {normalized}"
        # Remplacer les espaces par des +
        return normalized.replace(' ', '+')
    
    def get_image_filename(self, product_uid: str, image_url: str, index: int = 0) -> str:
        """G√©n√®re un nom de fichier pour l'image"""
        # Extraire l'extension de l'URL
        parsed = urlparse(image_url)
        path = parsed.path.lower()
        
        # D√©terminer l'extension
        if '.jpg' in path or '.jpeg' in path:
            ext = 'jpg'
        elif '.png' in path:
            ext = 'png'
        elif '.webp' in path:
            ext = 'webp'
        elif '.gif' in path:
            ext = 'gif'
        else:
            ext = 'jpg'  # Par d√©faut
        
        # G√©n√©rer le nom avec UID + index
        if index == 0:
            return f"{product_uid}_scraped.{ext}"
        else:
            return f"{product_uid}_scraped_{index}.{ext}"
    
    def download_image(self, image_url: str, filename: str) -> bool:
        """T√©l√©charge une image"""
        try:
            response = requests.get(image_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # V√©rifier que c'est bien une image
            content_type = response.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                print(f"‚ö†Ô∏è  Contenu non-image: {content_type}")
                return False
            
            # V√©rifier la taille minimale (√©viter les pixels de tracking)
            if len(response.content) < 1024:  # Moins de 1KB
                print(f"‚ö†Ô∏è  Image trop petite: {len(response.content)} bytes")
                return False
            
            # Sauvegarder l'image
            filepath = self.scraped_path / filename
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"‚úÖ Image t√©l√©charg√©e: {filename} ({len(response.content)} bytes)")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur t√©l√©chargement {image_url}: {e}")
            return False
    
    def extract_images_from_page(self, url: str, selectors: List[str]) -> List[str]:
        """Extrait les URLs d'images d'une page"""
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            image_urls = []
            
            # Essayer chaque s√©lecteur
            for selector in selectors:
                images = soup.select(selector)
                for img in images:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                    if src:
                        # Convertir en URL absolue
                        abs_url = urljoin(url, src)
                        if abs_url not in image_urls:
                            image_urls.append(abs_url)
            
            # Fallback: chercher toutes les images de bonne taille
            if not image_urls:
                all_images = soup.find_all('img')
                for img in all_images:
                    src = img.get('src') or img.get('data-src')
                    if src:
                        abs_url = urljoin(url, src)
                        # Filtrer par taille et nom
                        if (any(size in src for size in ['large', 'big', 'main', 'hero', 'product']) or
                            any(dim in src for dim in ['800', '600', '1000', '1200'])):
                            if abs_url not in image_urls:
                                image_urls.append(abs_url)
            
            return image_urls[:5]  # Limiter √† 5 images max
            
        except Exception as e:
            print(f"‚ùå Erreur extraction {url}: {e}")
            return []
    
    def search_google_images(self, query: str) -> List[str]:
        """Recherche d'images via Google (m√©thode simple)"""
        try:
            # Note: Google Images n√©cessite des m√©thodes plus sophistiqu√©es
            # pour un usage en production (API, Selenium, etc.)
            search_url = f"https://www.google.com/search?tbm=isch&q={query}"
            
            response = requests.get(search_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraction simple des URLs d'images
            image_urls = []
            img_tags = soup.find_all('img')
            
            for img in img_tags:
                src = img.get('src')
                if src and 'gstatic.com' not in src and src.startswith('http'):
                    image_urls.append(src)
                    if len(image_urls) >= 3:
                        break
            
            return image_urls
            
        except Exception as e:
            print(f"‚ùå Erreur recherche Google: {e}")
            return []
    
    def scrape_product_images(self, product_name: str, manufacturer: str = "", 
                            product_uid: str = "", max_images: int = 3) -> Dict:
        """
        Scrape les images d'un produit
        
        Args:
            product_name: Nom du produit
            manufacturer: Fabricant (optionnel)
            product_uid: UID du produit pour nommer les fichiers
            max_images: Nombre maximum d'images √† t√©l√©charger
        
        Returns:
            Dict avec les r√©sultats du scraping
        """
        print(f"\nüîç Recherche d'images pour: {product_name} ({manufacturer})")
        
        if not product_uid:
            product_uid = hashlib.md5(f"{product_name}_{manufacturer}".encode()).hexdigest()[:8]
        
        results = {
            'product_name': product_name,
            'manufacturer': manufacturer,
            'product_uid': product_uid,
            'scraped_images': [],
            'errors': [],
            'success': False
        }
        
        # Normaliser la requ√™te de recherche
        search_query = self.normalize_product_name(product_name, manufacturer)
        print(f"üîç Requ√™te de recherche: {search_query}")
        
        downloaded_count = 0
        
        # 1. Essayer les sites de fabricants sp√©cifiques
        if manufacturer.lower() in self.manufacturer_sites:
            print(f"üè≠ Recherche sur le site {manufacturer}...")
            site_info = self.manufacturer_sites[manufacturer.lower()]
            
            try:
                search_url = site_info["search_url"].format(query=search_query)
                image_urls = self.extract_images_from_page(search_url, site_info["img_selectors"])
                
                for i, img_url in enumerate(image_urls):
                    if downloaded_count >= max_images:
                        break
                    
                    filename = self.get_image_filename(product_uid, img_url, i)
                    if self.download_image(img_url, filename):
                        results['scraped_images'].append({
                            'filename': filename,
                            'source_url': img_url,
                            'source': f"{manufacturer}_official"
                        })
                        downloaded_count += 1
                    
                    time.sleep(self.delay_between_requests)
                    
            except Exception as e:
                error_msg = f"Erreur site {manufacturer}: {e}"
                results['errors'].append(error_msg)
                print(f"‚ùå {error_msg}")
        
        # 2. Recherche Google Images si pas assez d'images
        if downloaded_count < max_images:
            print("üîç Recherche Google Images...")
            try:
                google_images = self.search_google_images(search_query)
                
                for i, img_url in enumerate(google_images):
                    if downloaded_count >= max_images:
                        break
                    
                    filename = self.get_image_filename(product_uid, img_url, downloaded_count)
                    if self.download_image(img_url, filename):
                        results['scraped_images'].append({
                            'filename': filename,
                            'source_url': img_url,
                            'source': 'google_images'
                        })
                        downloaded_count += 1
                    
                    time.sleep(self.delay_between_requests)
                    
            except Exception as e:
                error_msg = f"Erreur Google Images: {e}"
                results['errors'].append(error_msg)
                print(f"‚ùå {error_msg}")
        
        results['success'] = downloaded_count > 0
        print(f"‚úÖ Images t√©l√©charg√©es: {downloaded_count}/{max_images}")
        
        return results

def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(description='MAGSAV Image Scraper')
    parser.add_argument('--product', required=True, help='Nom du produit')
    parser.add_argument('--manufacturer', help='Fabricant du produit')
    parser.add_argument('--uid', help='UID du produit')
    parser.add_argument('--max-images', type=int, default=3, help='Nombre max d\'images')
    parser.add_argument('--medias-path', default='medias', help='Chemin vers le dossier medias')
    parser.add_argument('--output-json', help='Fichier JSON de sortie')
    
    args = parser.parse_args()
    
    # Cr√©er le scraper
    scraper = ImageScraper(args.medias_path)
    
    # Scraper les images
    results = scraper.scrape_product_images(
        product_name=args.product,
        manufacturer=args.manufacturer or "",
        product_uid=args.uid or "",
        max_images=args.max_images
    )
    
    # Afficher les r√©sultats
    print(f"\nüìä R√âSULTATS:")
    print(f"   Produit: {results['product_name']}")
    print(f"   Fabricant: {results['manufacturer']}")
    print(f"   Images trouv√©es: {len(results['scraped_images'])}")
    print(f"   Erreurs: {len(results['errors'])}")
    print(f"   Succ√®s: {results['success']}")
    
    if results['scraped_images']:
        print(f"\nüìÅ Images t√©l√©charg√©es:")
        for img in results['scraped_images']:
            print(f"   ‚Ä¢ {img['filename']} (source: {img['source']})")
    
    if results['errors']:
        print(f"\n‚ö†Ô∏è  Erreurs:")
        for error in results['errors']:
            print(f"   ‚Ä¢ {error}")
    
    # Sauvegarder en JSON si demand√©
    if args.output_json:
        with open(args.output_json, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ R√©sultats sauvegard√©s: {args.output_json}")
    
    # Code de sortie
    sys.exit(0 if results['success'] else 1)

if __name__ == "__main__":
    main()